---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Distributional reinforcement learning for inventory management in multi-echelon
  supply chains
subtitle: ''
summary: ''
authors:
- Guoquan Wu
<<<<<<< HEAD
- Miguel Ángel de Carvalho Servia
=======
- miguel
>>>>>>> 45fef293bad45213e1e49db40da7d42cc67812b3
- Max Mowbray
tags:
- Distributional reinforcement learning
- Optimal control
- Inventory management
- Multi-echelon supply chains
- Machine learning
categories: []
date: '2023-01-01'
lastmod: 2023-05-17T12:15:01+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-05-17T11:36:43.863247Z'
publication_types:
- '2'
abstract: Reinforcement Learning (RL) is an effective method to solve stochastic sequential
  decision-making problems. This is a problem description common to supply chain operations,
  however, most RL algorithms are tailored for game-based benchmarks. Here, we propose
  a deep RL method tailored for supply chain problems. The proposed algorithm deploys
  a derivative free approach to balance exploration and exploitation of the neural
  policy’s parameter space, providing means to avoid low quality local optima. Furthermore,
  the method allows consideration of risk-sensitive formulations to learn a policy
  that optimizes, for example, the conditional value-at-risk. The capabilities of
  our algorithm are tested on a multi-echelon supply chain problem, and several combinatorial
  optimization problems. The results empirically demonstrate the method’s improved
  sample efficiency compared to the benchmark algorithm proximal policy optimization,
  and superior performance to shrinking horizon mixed integer formulations. Additionally,
  its risk-sensitive policy can offer protection from low probability, high severity
  scenarios. Finally, we provide a sensitivity analysis for technical intuition.
publication: '*Digital Chemical Engineering*'
doi: https://doi.org/10.1016/j.dche.2022.100073
links:
- name: URL
  url: https://www.sciencedirect.com/science/article/pii/S2772508122000643
---
