@incollection{sitter_convex_2022,
 abstract = {Over the last decade, Reinforcement Learning (RL) has received significant attention as it promises novel and efficient solutions to complex control problems. This work builds on model-free RL, namely Q-learning, to determine optimal control policies for nonlinear, complex biochemical processes. We propose convex functions instead of deep neural networks as state-action value function approximators to reduce computational complexity. A convex Q-function surrogate is trained using semidefinite programming. The surrogate is then minimized to determine the optimal control action. This results in 75.3% lower computational time compared with deep Q-networks. By alleviating the computational burden of traditional RL approximation functions, this work addresses one of the major obstacles for the successful implementation of RL to real-world engineering applications.},
 author = {Sitter, Sophie and van de Berg, Damien and Mowbray, Max and del Rio Chanona, Antonio and Petsagkourakis, Panagiotis},
 booktitle = {Computer Aided Chemical Engineering},
 doi = {10.1016/B978-0-323-85159-6.50056-7},
 editor = {Yamashita, Yoshiyuki and Kano, Manabu},
 file = {ScienceDirect Snapshot:/Users/busesibelkorkmaz/Zotero/storage/4VPNUSJ5/B9780323851596500567.html:text/html},
 keywords = {convex Q-learning, data-driven batch optimization, dynamic process control, machine learning, semi-definite programming},
 language = {en},
 month = {January},
 pages = {337--342},
 publisher = {Elsevier},
 series = {14 International Symposium on Process Systems Engineering},
 shorttitle = {Convex Q-learning},
 title = {Convex Q-learning: Reinforcement learning through convex programming},
 url = {https://www.sciencedirect.com/science/article/pii/B9780323851596500567},
 urldate = {2023-05-16},
 volume = {49},
 year = {2022}
}

