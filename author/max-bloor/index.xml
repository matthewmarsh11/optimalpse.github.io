<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Max Bloor | OptiML PSE</title><link>https://optimalpse.github.io/author/max-bloor/</link><atom:link href="https://optimalpse.github.io/author/max-bloor/index.xml" rel="self" type="application/rss+xml"/><description>Max Bloor</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 07 May 2024 00:00:00 +0000</lastBuildDate><image><url>https://optimalpse.github.io/author/max-bloor/avatar_hu08287478c51bf72a1b2593382b155b0d_90961_270x270_fill_q90_lanczos_center.jpg</url><title>Max Bloor</title><link>https://optimalpse.github.io/author/max-bloor/</link></image><item><title>Reinforcement Learning for PSE</title><link>https://optimalpse.github.io/post/reinforcement-learning/</link><pubDate>Tue, 07 May 2024 00:00:00 +0000</pubDate><guid>https://optimalpse.github.io/post/reinforcement-learning/</guid><description>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/post/reinforcement-learning/featured_hu259738f748945f634c03d5b7c7684f95_59336_2c6444a9aca5d7b2a8ae3cb9edbd3056.webp 400w,
/post/reinforcement-learning/featured_hu259738f748945f634c03d5b7c7684f95_59336_c3f2fdf819f76f48446ca143503f9374.webp 760w,
/post/reinforcement-learning/featured_hu259738f748945f634c03d5b7c7684f95_59336_1200x1200_fit_q90_h2_lanczos.webp 1200w"
src="https://optimalpse.github.io/post/reinforcement-learning/featured_hu259738f748945f634c03d5b7c7684f95_59336_2c6444a9aca5d7b2a8ae3cb9edbd3056.webp"
width="760"
height="477"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="" srcset="
/post/reinforcement-learning/pc_hud3c0e97c9a69add1fe991c938a75f802_147783_9ce285d4144c934bcdd7b0ac1b184fd0.webp 400w,
/post/reinforcement-learning/pc_hud3c0e97c9a69add1fe991c938a75f802_147783_eb844060a583f00ad0a14e9e725904ba.webp 760w,
/post/reinforcement-learning/pc_hud3c0e97c9a69add1fe991c938a75f802_147783_1200x1200_fit_q90_h2_lanczos_3.webp 1200w"
src="https://optimalpse.github.io/post/reinforcement-learning/pc_hud3c0e97c9a69add1fe991c938a75f802_147783_9ce285d4144c934bcdd7b0ac1b184fd0.webp"
width="760"
height="449"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;div style="text-align: justify">
The chemical industry requires efficient control systems to operate at the border of process constraints while optimizing for profit, safety, and sustainability. Reinforcement Learning (RL) is a control philosophy that aims to address the complex control problems present in chemical systems. RL utilises plant data to improve its control performance and has several advantages over other control strategies such as its offline inference time and flexibility to adapt to changing plant conditions. However, RL is an active area of research to develop and implement algorithms suitable for industrial useglobal optimum due to its inherent robustness.
&lt;/div></description></item></channel></rss>